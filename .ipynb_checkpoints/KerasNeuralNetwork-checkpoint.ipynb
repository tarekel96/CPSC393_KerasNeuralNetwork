{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd25cc",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fbf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# packages for building the model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras import losses \n",
    "from tensorflow.keras import metrics\n",
    "# packages for training model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "# misc package\n",
    "import category_encoders as category_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d3a138",
   "metadata": {},
   "source": [
    "# Import and view raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a32acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fee502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n",
       "       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope',\n",
       "       'HeartDisease'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2586a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83efb09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 918 samples, 12 features\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e281b0f",
   "metadata": {},
   "source": [
    "# Preprocessing of Data\n",
    "- Check for null values\n",
    "- Check for duplicated values\n",
    "- convert categorical data to numerical data\n",
    "- Normalize data\n",
    "- Split data into train and test samples/labels\n",
    "- Convert datasets to numpy arrays to be able to pass data to Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555aba2d",
   "metadata": {},
   "source": [
    "## Preprocessing data: Check for null and/or duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4692f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               0\n",
       "Sex               0\n",
       "ChestPainType     0\n",
       "RestingBP         0\n",
       "Cholesterol       0\n",
       "FastingBS         0\n",
       "RestingECG        0\n",
       "MaxHR             0\n",
       "ExerciseAngina    0\n",
       "Oldpeak           0\n",
       "ST_Slope          0\n",
       "HeartDisease      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values - there is no missing data\n",
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad1be5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicate values - there is no duplicated data\n",
    "data.duplicated().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8151f8",
   "metadata": {},
   "source": [
    "## Preprocessing data: View unique values for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205a35c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n",
      "Values: ['M' 'F']\n",
      "\n",
      "\n",
      "ChestPainType\n",
      "Values: ['ATA' 'NAP' 'ASY' 'TA']\n",
      "\n",
      "\n",
      "RestingECG\n",
      "Values: ['Normal' 'ST' 'LVH']\n",
      "\n",
      "\n",
      "ExerciseAngina\n",
      "Values: ['N' 'Y']\n",
      "\n",
      "\n",
      "ST_Slope\n",
      "Values: ['Up' 'Flat' 'Down']\n",
      "\n",
      "\n",
      "The following categories: ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'] shall be converted to numerical data via pd.get_dummies\n"
     ]
    }
   ],
   "source": [
    "category_columns = []\n",
    "\n",
    "# viewing the unique values, number of dimensions, and shape of each column in the data frame\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        print(f'{col}')\n",
    "        print(f'Values: {data[col].unique()}')\n",
    "        print('\\n')\n",
    "        category_columns.append(col)\n",
    "print(f'The following categories: {category_columns} shall be converted to numerical data via pd.get_dummies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24dd083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels - 0 does not have heart disease, 1 does have heart disease\n",
    "data['HeartDisease'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98373e04",
   "metadata": {},
   "source": [
    "## Preprocessing data: Converting categorical values into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210ff124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 21 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Age                918 non-null    int64  \n",
      " 1   RestingBP          918 non-null    int64  \n",
      " 2   Cholesterol        918 non-null    int64  \n",
      " 3   FastingBS          918 non-null    int64  \n",
      " 4   MaxHR              918 non-null    int64  \n",
      " 5   Oldpeak            918 non-null    float64\n",
      " 6   HeartDisease       918 non-null    int64  \n",
      " 7   Sex_F              918 non-null    uint8  \n",
      " 8   Sex_M              918 non-null    uint8  \n",
      " 9   ChestPainType_ASY  918 non-null    uint8  \n",
      " 10  ChestPainType_ATA  918 non-null    uint8  \n",
      " 11  ChestPainType_NAP  918 non-null    uint8  \n",
      " 12  ChestPainType_TA   918 non-null    uint8  \n",
      " 13  RestingECG_LVH     918 non-null    uint8  \n",
      " 14  RestingECG_Normal  918 non-null    uint8  \n",
      " 15  RestingECG_ST      918 non-null    uint8  \n",
      " 16  ExerciseAngina_N   918 non-null    uint8  \n",
      " 17  ExerciseAngina_Y   918 non-null    uint8  \n",
      " 18  ST_Slope_Down      918 non-null    uint8  \n",
      " 19  ST_Slope_Flat      918 non-null    uint8  \n",
      " 20  ST_Slope_Up        918 non-null    uint8  \n",
      "dtypes: float64(1), int64(6), uint8(14)\n",
      "memory usage: 62.9 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "    Age  RestingBP  Cholesterol  FastingBS  MaxHR  Oldpeak  HeartDisease  \\\n",
       " 0   40        140          289          0    172      0.0             0   \n",
       " 1   49        160          180          0    156      1.0             1   \n",
       " \n",
       "    Sex_F  Sex_M  ChestPainType_ASY  ...  ChestPainType_NAP  ChestPainType_TA  \\\n",
       " 0      0      1                  0  ...                  0                 0   \n",
       " 1      1      0                  0  ...                  1                 0   \n",
       " \n",
       "    RestingECG_LVH  RestingECG_Normal  RestingECG_ST  ExerciseAngina_N  \\\n",
       " 0               0                  1              0                 1   \n",
       " 1               0                  1              0                 1   \n",
       " \n",
       "    ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  ST_Slope_Up  \n",
       " 0                 0              0              0            1  \n",
       " 1                 0              0              1            0  \n",
       " \n",
       " [2 rows x 21 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the nomimnal categorical label values to numerical values\n",
    "\n",
    "# get_dummies method\n",
    "mod_data = data.copy()\n",
    "mod_data = pd.get_dummies(mod_data, columns=category_columns)\n",
    "mod_data.info(), mod_data.head(2)\n",
    "\n",
    "# encoder = category_encoder.BinaryEncoder(cols = category_columns)\n",
    "# data_mod = data.copy()\n",
    "# df_category_encorder = encoder.fit_transform(data_mod)\n",
    "# df_category_encorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cca180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak',\n",
       "       'HeartDisease', 'Sex_F', 'Sex_M', 'ChestPainType_ASY',\n",
       "       'ChestPainType_ATA', 'ChestPainType_NAP', 'ChestPainType_TA',\n",
       "       'RestingECG_LVH', 'RestingECG_Normal', 'RestingECG_ST',\n",
       "       'ExerciseAngina_N', 'ExerciseAngina_Y', 'ST_Slope_Down',\n",
       "       'ST_Slope_Flat', 'ST_Slope_Up'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ded9412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Age',\n",
       "  'RestingBP',\n",
       "  'Cholesterol',\n",
       "  'FastingBS',\n",
       "  'MaxHR',\n",
       "  'Oldpeak',\n",
       "  'Sex_F',\n",
       "  'Sex_M',\n",
       "  'ChestPainType_ASY',\n",
       "  'ChestPainType_ATA',\n",
       "  'ChestPainType_NAP',\n",
       "  'ChestPainType_TA',\n",
       "  'RestingECG_LVH',\n",
       "  'RestingECG_Normal',\n",
       "  'RestingECG_ST',\n",
       "  'ExerciseAngina_N',\n",
       "  'ExerciseAngina_Y',\n",
       "  'ST_Slope_Down',\n",
       "  'ST_Slope_Flat',\n",
       "  'ST_Slope_Up'],\n",
       " 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [col for col in mod_data.columns if col != 'HeartDisease']\n",
    "output = ['HeartDisease']\n",
    "features, len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9be01",
   "metadata": {},
   "source": [
    "## Preprocessing data: Convert data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c79516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, (918, 20), (918, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(mod_data[features])\n",
    "y = np.array(mod_data[output])\n",
    "\n",
    "len(features), X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04001032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y), X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb0f08",
   "metadata": {},
   "source": [
    "## Preprocessing data: Splitting data into train and test samples/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c54c75bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((734, 20), (734, 1), 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataframe into train samples/labels and test samples/labels\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "train_samples.shape, train_labels.shape, len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23ad99",
   "metadata": {},
   "source": [
    "# Build the Neural Network Model with Keras Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "754a5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a Sequential model - linear stack of layers\n",
    "# init the model\n",
    "model = Sequential()\n",
    "# add Dense layers to the models\n",
    "# units = number of nodes, input_shape = tensor shape the input layer expect (inits weights); activation - activation function\n",
    "model.add(Dense(units=16, activation='relu', input_shape=(20, )))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "# use sigmoid in last layer because it is a binary classification problem\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d76481",
   "metadata": {},
   "source": [
    "## Visualization of Neural Network Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a89602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 625\n",
      "Trainable params: 625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8eef5",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "- Assign loss function: the function that is minimized by the optimizer\n",
    "- Assign optimizer function: how the model learns and minimizes the loss function \n",
    "- Choose metrics: used to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e026510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation step - 1) loss function 2) optimizer 3) Metrics to monitor during training and testing\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),loss=losses.binary_crossentropy,metrics=[\"acc\"])\n",
    "\n",
    "# alt ways to construct optimizer\n",
    "# opt = Adam(learning_rate=0.01) # defining the optimizer\n",
    "# model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25305e5b",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "- Assign the train samples as x with their train labels as y\n",
    "- Assign batch size, the number of samples that will be propagated through the network\n",
    "- Assign epochs, the number of iterations the model will run through the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21d04d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "37/37 [==============================] - 1s 839us/step - loss: 2.2997 - acc: 0.6935\n",
      "Epoch 2/30\n",
      "37/37 [==============================] - 0s 993us/step - loss: 1.9204 - acc: 0.6853\n",
      "Epoch 3/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.6935 - acc: 0.6880\n",
      "Epoch 4/30\n",
      "37/37 [==============================] - 0s 993us/step - loss: 1.3809 - acc: 0.7166\n",
      "Epoch 5/30\n",
      "37/37 [==============================] - 0s 820us/step - loss: 1.0859 - acc: 0.6948\n",
      "Epoch 6/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.0055 - acc: 0.6935\n",
      "Epoch 7/30\n",
      "37/37 [==============================] - 0s 964us/step - loss: 0.8901 - acc: 0.7030\n",
      "Epoch 8/30\n",
      "37/37 [==============================] - 0s 925us/step - loss: 0.8764 - acc: 0.6935\n",
      "Epoch 9/30\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.7120 - acc: 0.7166\n",
      "Epoch 10/30\n",
      "37/37 [==============================] - 0s 892us/step - loss: 0.6887 - acc: 0.7003\n",
      "Epoch 11/30\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.6940 - acc: 0.6907\n",
      "Epoch 12/30\n",
      "37/37 [==============================] - 0s 632us/step - loss: 0.6008 - acc: 0.7357\n",
      "Epoch 13/30\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.6218 - acc: 0.7439\n",
      "Epoch 14/30\n",
      "37/37 [==============================] - 0s 602us/step - loss: 0.5951 - acc: 0.7507\n",
      "Epoch 15/30\n",
      "37/37 [==============================] - 0s 675us/step - loss: 0.4996 - acc: 0.7943\n",
      "Epoch 16/30\n",
      "37/37 [==============================] - 0s 728us/step - loss: 0.5334 - acc: 0.7425\n",
      "Epoch 17/30\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.5566 - acc: 0.7766\n",
      "Epoch 18/30\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.5675 - acc: 0.7589\n",
      "Epoch 19/30\n",
      "37/37 [==============================] - 0s 666us/step - loss: 0.4944 - acc: 0.7875\n",
      "Epoch 20/30\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4525 - acc: 0.8093\n",
      "Epoch 21/30\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.5073 - acc: 0.8093\n",
      "Epoch 22/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4776 - acc: 0.8188\n",
      "Epoch 23/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4944 - acc: 0.7793\n",
      "Epoch 24/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4417 - acc: 0.8215\n",
      "Epoch 25/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4661 - acc: 0.8038\n",
      "Epoch 26/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4601 - acc: 0.8215\n",
      "Epoch 27/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4728 - acc: 0.7929\n",
      "Epoch 28/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4484 - acc: 0.8229\n",
      "Epoch 29/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4516 - acc: 0.8283\n",
      "Epoch 30/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4887 - acc: 0.8052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8ccc0b9130>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model by fitting the normalized training data\n",
    "history = model.fit(x=train_samples, y=train_labels, batch_size=20, epochs=30)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fecb0",
   "metadata": {},
   "source": [
    "# Evaluate the Model's performance\n",
    "- The evaluate method takes in a test sample numpy array and their associated test labels\n",
    "- Returns the loss value & metrics value (accuracy score) for the model in test mode\n",
    "- Loss is the scalar value that is attempted to be minimized during training of the model. The lower the loss, the closer our predictions are to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5193d7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 879us/step - loss: 0.7382 - acc: 0.7174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7382122278213501, 0.717391312122345]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(test_samples, test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f75e3",
   "metadata": {},
   "source": [
    "# Further Experiments\n",
    "- ## Hidden Layers\n",
    "    - Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\n",
    "- ## Hidden Units\n",
    "    - Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on. \n",
    "- ## Loss Functions\n",
    "    - Try using the mse loss function instead of binary_crossentropy. \n",
    "- ## Activation Function\n",
    "    - Try using the tanh activation (an activation that was popular in the early days of neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc599841",
   "metadata": {},
   "source": [
    "## Reusable function for creating Keras Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08258789",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LAYERS = [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )),\n",
    "          Dense(units=32, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "]\n",
    "DEFAULT_OPTIMIZER = optimizers.RMSprop(learning_rate=0.001)\n",
    "DEFAULT_LOSS = losses.binary_crossentropy\n",
    "DEFAULT_METRICS = [\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67dbbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(train_samples, \n",
    "                          train_labels,\n",
    "                          layers = DEFAULT_LAYERS, \n",
    "                          optimizer=DEFAULT_OPTIMIZER,\n",
    "                          loss=DEFAULT_LOSS,\n",
    "                          metrics=DEFAULT_METRICS):\n",
    "    model = Sequential(layers)\n",
    "    model.compile(optimizer=optimizer, loss=loss,metrics=metrics)\n",
    "    model.fit(x=train_samples, y=train_labels, batch_size=20, epochs=30, verbose=0)\n",
    "    return model\n",
    "\n",
    "def get_nn_results(model, test_samples, test_labels):\n",
    "    return model.evaluate(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7a163",
   "metadata": {},
   "source": [
    "## Further Experiments: Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb03dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {\n",
    "    # 1 hidden layer\n",
    "    '1': [\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 2 hidden layers\n",
    "    '2': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 3 hidden layers\n",
    "    '3': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ]\n",
    "}\n",
    "results_layers_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad86f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 681us/step - loss: 1.6108 - acc: 0.7011\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3959 - acc: 0.8152\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4391 - acc: 0.8370\n",
      "\n",
      "\n",
      "Achieved 1.6108 loss 0.7011 accuracy with 1 layer(s)\n",
      "Achieved 0.3959 loss 0.8152 accuracy with 2 layer(s)\n",
      "Achieved 0.4391 loss 0.8370 accuracy with 3 layer(s)\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "for key, layers in layers_dict.items():\n",
    "    curr_model = create_neural_network(train_samples, train_labels, layers)\n",
    "    results_layers_dict[key] = get_nn_results(curr_model, test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_layers_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with {key} layer(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4853c82",
   "metadata": {},
   "source": [
    "## Further Experiments: Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a94d7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dict = {\n",
    "    # 16 hidden layer\n",
    "    '16': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 32 hidden layers\n",
    "    '32': [\n",
    "          Dense(units=32, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=32, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 64 hidden layers\n",
    "    '64': [\n",
    "          Dense(units=64, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=64, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ]\n",
    "}\n",
    "results_units_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ba605e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 789us/step - loss: 0.4622 - acc: 0.7935\n",
      "6/6 [==============================] - 0s 739us/step - loss: 0.4520 - acc: 0.8315\n",
      "6/6 [==============================] - 0s 689us/step - loss: 0.9110 - acc: 0.6196\n",
      "\n",
      "\n",
      "Achieved 0.4622 loss 0.7935 accuracy with 16 units/nodes per deep layer\n",
      "Achieved 0.4520 loss 0.8315 accuracy with 32 units/nodes per deep layer\n",
      "Achieved 0.9110 loss 0.6196 accuracy with 64 units/nodes per deep layer\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "for key, layers in units_dict.items():\n",
    "    curr_model = create_neural_network(train_samples, train_labels, layers)\n",
    "    results_units_dict[key] = get_nn_results(curr_model, test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_units_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with {key} units/nodes per deep layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6a0ac",
   "metadata": {},
   "source": [
    "## Further Experiments: Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b4bbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = ['binary_crossentropy', 'hinge', 'squared_hinge']\n",
    "results_loss_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf29e30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 699us/step - loss: 0.5507 - acc: 0.7826\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7288 - acc: 0.7609\n",
      "6/6 [==============================] - 0s 981us/step - loss: 0.7773 - acc: 0.7554\n",
      "\n",
      "\n",
      "Achieved 0.5507 loss 0.7826 accuracy with the binary_crossentropy loss function\n",
      "Achieved 0.7288 loss 0.7609 accuracy with the hinge loss function\n",
      "Achieved 0.7773 loss 0.7554 accuracy with the squared_hinge loss function\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "for loss_func in loss_functions:\n",
    "    curr_model = create_neural_network(train_samples, train_labels, loss=loss_func)\n",
    "    results_loss_dict[loss_func] = get_nn_results(curr_model, test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_loss_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with the {key} loss function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884a3b",
   "metadata": {},
   "source": [
    "## Further Experiments: Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b29a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions_dict = {\n",
    "    # sigmoid\n",
    "    'sigmoid': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # tanh\n",
    "    'tanh': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='tanh')\n",
    "         ],\n",
    "    # relu\n",
    "    'relu': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='relu')\n",
    "         ]\n",
    "}\n",
    "results_activation_functions_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a983c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3497 - acc: 0.8804\n",
      "6/6 [==============================] - 0s 812us/step - loss: 7.7904 - acc: 0.4891\n",
      "6/6 [==============================] - 0s 766us/step - loss: 7.7904 - acc: 0.4891\n",
      "\n",
      "\n",
      "Achieved 0.3497 loss 0.8804 accuracy with the sigmoid activation function.\n",
      "Achieved 7.7904 loss 0.4891 accuracy with the tanh activation function.\n",
      "Achieved 7.7904 loss 0.4891 accuracy with the relu activation function.\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "for key, layers in activation_functions_dict.items():\n",
    "    curr_model = create_neural_network(train_samples, train_labels, layers)\n",
    "    results_activation_functions_dict[key] = get_nn_results(curr_model, test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_activation_functions_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with the {key} activation function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c965e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
