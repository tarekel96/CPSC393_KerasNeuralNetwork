{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd25cc",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fbf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# packages for building the model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras import losses \n",
    "from tensorflow.keras import metrics\n",
    "# packages for training model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "# misc package\n",
    "import category_encoders as category_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d3a138",
   "metadata": {},
   "source": [
    "# Import and view raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a32acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fee502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n",
       "       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope',\n",
       "       'HeartDisease'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2586a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83efb09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 918 samples, 12 features\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e281b0f",
   "metadata": {},
   "source": [
    "# Preprocessing of Data\n",
    "- Check for null values\n",
    "- Check for duplicated values\n",
    "- convert categorical data to numerical data\n",
    "- Normalize data\n",
    "- Split data into train and test samples/labels\n",
    "- Convert datasets to numpy arrays to be able to pass data to Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555aba2d",
   "metadata": {},
   "source": [
    "## Preprocessing data: Check for null and/or duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4692f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               0\n",
       "Sex               0\n",
       "ChestPainType     0\n",
       "RestingBP         0\n",
       "Cholesterol       0\n",
       "FastingBS         0\n",
       "RestingECG        0\n",
       "MaxHR             0\n",
       "ExerciseAngina    0\n",
       "Oldpeak           0\n",
       "ST_Slope          0\n",
       "HeartDisease      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values - there is no missing data\n",
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad1be5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicate values - there is no duplicated data\n",
    "data.duplicated().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8151f8",
   "metadata": {},
   "source": [
    "## Preprocessing data: View unique values for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "205a35c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n",
      "Values: ['M' 'F']\n",
      "\n",
      "\n",
      "ChestPainType\n",
      "Values: ['ATA' 'NAP' 'ASY' 'TA']\n",
      "\n",
      "\n",
      "RestingECG\n",
      "Values: ['Normal' 'ST' 'LVH']\n",
      "\n",
      "\n",
      "ExerciseAngina\n",
      "Values: ['N' 'Y']\n",
      "\n",
      "\n",
      "ST_Slope\n",
      "Values: ['Up' 'Flat' 'Down']\n",
      "\n",
      "\n",
      "The following categories: ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'] shall be converted to numerical data via pd.get_dummies\n"
     ]
    }
   ],
   "source": [
    "category_columns = []\n",
    "\n",
    "# viewing the unique values, number of dimensions, and shape of each column in the data frame\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        print(f'{col}')\n",
    "        print(f'Values: {data[col].unique()}')\n",
    "        print('\\n')\n",
    "        category_columns.append(col)\n",
    "print(f'The following categories: {category_columns} shall be converted to numerical data via pd.get_dummies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dd083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels - 0 does not have heart disease, 1 does have heart disease\n",
    "data['HeartDisease'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98373e04",
   "metadata": {},
   "source": [
    "## Preprocessing data: Converting categorical values into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "210ff124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>...</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  RestingBP  Cholesterol  FastingBS  MaxHR  Oldpeak  HeartDisease  \\\n",
       "0   40        140          289          0    172      0.0             0   \n",
       "1   49        160          180          0    156      1.0             1   \n",
       "\n",
       "   Sex_F  Sex_M  ChestPainType_ASY  ...  ChestPainType_NAP  ChestPainType_TA  \\\n",
       "0      0      1                  0  ...                  0                 0   \n",
       "1      1      0                  0  ...                  1                 0   \n",
       "\n",
       "   RestingECG_LVH  RestingECG_Normal  RestingECG_ST  ExerciseAngina_N  \\\n",
       "0               0                  1              0                 1   \n",
       "1               0                  1              0                 1   \n",
       "\n",
       "   ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  ST_Slope_Up  \n",
       "0                 0              0              0            1  \n",
       "1                 0              0              1            0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the nomimnal categorical label values to numerical values\n",
    "\n",
    "# get_dummies method (one hot encoding technique for multi-value categorical variables)\n",
    "mod_data = data.copy()\n",
    "mod_data = pd.get_dummies(mod_data, columns=category_columns)\n",
    "mod_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cca180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak',\n",
       "       'HeartDisease', 'Sex_F', 'Sex_M', 'ChestPainType_ASY',\n",
       "       'ChestPainType_ATA', 'ChestPainType_NAP', 'ChestPainType_TA',\n",
       "       'RestingECG_LVH', 'RestingECG_Normal', 'RestingECG_ST',\n",
       "       'ExerciseAngina_N', 'ExerciseAngina_Y', 'ST_Slope_Down',\n",
       "       'ST_Slope_Flat', 'ST_Slope_Up'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ded9412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Age',\n",
       "  'RestingBP',\n",
       "  'Cholesterol',\n",
       "  'FastingBS',\n",
       "  'MaxHR',\n",
       "  'Oldpeak',\n",
       "  'Sex_F',\n",
       "  'Sex_M',\n",
       "  'ChestPainType_ASY',\n",
       "  'ChestPainType_ATA',\n",
       "  'ChestPainType_NAP',\n",
       "  'ChestPainType_TA',\n",
       "  'RestingECG_LVH',\n",
       "  'RestingECG_Normal',\n",
       "  'RestingECG_ST',\n",
       "  'ExerciseAngina_N',\n",
       "  'ExerciseAngina_Y',\n",
       "  'ST_Slope_Down',\n",
       "  'ST_Slope_Flat',\n",
       "  'ST_Slope_Up'],\n",
       " 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating features from output (HeartDisease)\n",
    "features = [col for col in mod_data.columns if col != 'HeartDisease']\n",
    "output = ['HeartDisease']\n",
    "features, len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9be01",
   "metadata": {},
   "source": [
    "## Preprocessing data: Convert data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c79516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, (918, 20), (918, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(mod_data[features])\n",
    "y = np.array(mod_data[output])\n",
    "\n",
    "len(features), X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04001032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y), X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb0f08",
   "metadata": {},
   "source": [
    "## Preprocessing data: Splitting data into train and test samples/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c54c75bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((734, 20), (734, 1), 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 2\n",
    "\n",
    "# split dataframe into train samples/labels and test samples/labels\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "train_samples.shape, train_labels.shape, len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ee29e",
   "metadata": {},
   "source": [
    "# Preprocessing data: Normalizing (z-scoring) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66af50fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.44897959, 0.8       , 0.        , ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.12244898, 0.75      , 0.35489221, ..., 0.        , 0.        ,\n",
       "         1.        ],\n",
       "        [0.67346939, 0.705     , 0.48424544, ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.46938776, 0.685     , 0.56218905, ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.67346939, 0.695     , 0.46932007, ..., 0.        , 0.        ,\n",
       "         1.        ],\n",
       "        [0.6122449 , 0.675     , 0.3681592 , ..., 0.        , 0.        ,\n",
       "         1.        ]]),\n",
       " (734, 20),\n",
       " array([[0.48979592, 0.64      , 0.        , ..., 0.        , 0.        ,\n",
       "         1.        ],\n",
       "        [0.30612245, 0.575     , 0.        , ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.87755102, 0.65      , 0.36650083, ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.71428571, 0.65      , 0.4212272 , ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.67346939, 0.65      , 0.        , ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.59183673, 0.6       , 0.58706468, ..., 0.        , 0.        ,\n",
       "         1.        ]]),\n",
       " (184, 20))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts the numerical values to all be within the range of [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=[0,1])\n",
    "# fit and transform train data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# transform but not fit test data to prevent test data bias leakage\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "# verify the data was transformed\n",
    "scaled_train_samples, scaled_train_samples.shape, scaled_test_samples, scaled_test_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23ad99",
   "metadata": {},
   "source": [
    "# Build the Neural Network Model with Keras Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "754a5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a Sequential model - linear stack of layers\n",
    "# init the model\n",
    "model = Sequential()\n",
    "# add Dense layers to the models\n",
    "# units = number of nodes, input_shape = tensor shape the input layer expect (inits weights); activation - activation function\n",
    "model.add(Dense(units=16, activation='relu', input_shape=(20, )))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "# use sigmoid in last layer because it is a binary classification problem\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d76481",
   "metadata": {},
   "source": [
    "## Visualization of Neural Network Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a89602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 625\n",
      "Trainable params: 625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8eef5",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "- Assign loss function: the function that is minimized by the optimizer\n",
    "- Assign optimizer function: how the model learns and minimizes the loss function \n",
    "- Choose metrics: used to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e026510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation step - 1) loss function 2) optimizer 3) Metrics to monitor during training and testing\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),loss=losses.binary_crossentropy,metrics=[\"accuracy\"])\n",
    "\n",
    "# alt ways to construct optimizer\n",
    "# opt = Adam(learning_rate=0.01) # defining the optimizer\n",
    "# model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25305e5b",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "- Assign the train samples as x with their train labels as y\n",
    "- Assign batch size, the number of samples that will be propagated through the network\n",
    "- Assign epochs, the number of iterations the model will run through the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21d04d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "37/37 [==============================] - 0s 703us/step - loss: 0.6031 - accuracy: 0.7520\n",
      "Epoch 2/30\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.4909 - accuracy: 0.8270\n",
      "Epoch 3/30\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.4123 - accuracy: 0.8420\n",
      "Epoch 4/30\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.3705 - accuracy: 0.8515\n",
      "Epoch 5/30\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.3537 - accuracy: 0.8529\n",
      "Epoch 6/30\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.3438 - accuracy: 0.8569\n",
      "Epoch 7/30\n",
      "37/37 [==============================] - 0s 976us/step - loss: 0.3373 - accuracy: 0.8638\n",
      "Epoch 8/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8624\n",
      "Epoch 9/30\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.3292 - accuracy: 0.8610\n",
      "Epoch 10/30\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3283 - accuracy: 0.8624\n",
      "Epoch 11/30\n",
      "37/37 [==============================] - 0s 960us/step - loss: 0.3260 - accuracy: 0.8665\n",
      "Epoch 12/30\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.3231 - accuracy: 0.8610\n",
      "Epoch 13/30\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.3228 - accuracy: 0.8692\n",
      "Epoch 14/30\n",
      "37/37 [==============================] - 0s 915us/step - loss: 0.3184 - accuracy: 0.8665\n",
      "Epoch 15/30\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.3206 - accuracy: 0.8678\n",
      "Epoch 16/30\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.3206 - accuracy: 0.8665\n",
      "Epoch 17/30\n",
      "37/37 [==============================] - 0s 955us/step - loss: 0.3174 - accuracy: 0.8651\n",
      "Epoch 18/30\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.3173 - accuracy: 0.8651\n",
      "Epoch 19/30\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.3165 - accuracy: 0.8665\n",
      "Epoch 20/30\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.3164 - accuracy: 0.8665\n",
      "Epoch 21/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8665\n",
      "Epoch 22/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3133 - accuracy: 0.8651\n",
      "Epoch 23/30\n",
      "37/37 [==============================] - 0s 928us/step - loss: 0.3113 - accuracy: 0.8665\n",
      "Epoch 24/30\n",
      "37/37 [==============================] - 0s 917us/step - loss: 0.3095 - accuracy: 0.8719\n",
      "Epoch 25/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8760\n",
      "Epoch 26/30\n",
      "37/37 [==============================] - 0s 968us/step - loss: 0.3079 - accuracy: 0.8665\n",
      "Epoch 27/30\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.3082 - accuracy: 0.8706\n",
      "Epoch 28/30\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.3081 - accuracy: 0.8665\n",
      "Epoch 29/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8706\n",
      "Epoch 30/30\n",
      "37/37 [==============================] - 0s 931us/step - loss: 0.3060 - accuracy: 0.8733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6baecc100>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model by fitting the normalized training data\n",
    "history = model.fit(x=scaled_train_samples, y=train_labels, batch_size=20, epochs=30)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fecb0",
   "metadata": {},
   "source": [
    "# Evaluate the Model's performance\n",
    "- The evaluate method takes in a test sample numpy array and their associated test labels\n",
    "- Returns the loss value & metrics value (accuracy score) for the model in test mode\n",
    "- Loss is the scalar value that is attempted to be minimized during training of the model. The lower the loss, the closer our predictions are to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5193d7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 742us/step - loss: 0.3765 - accuracy: 0.8315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37645289301872253, 0.83152174949646]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(scaled_test_samples, test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f75e3",
   "metadata": {},
   "source": [
    "# Further Experiments\n",
    "- ## Hidden Layers\n",
    "    - Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\n",
    "- ## Hidden Units\n",
    "    - Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on. \n",
    "- ## Loss Functions\n",
    "    - Try using the mse loss function instead of binary_crossentropy. \n",
    "- ## Activation Function\n",
    "    - Try using the tanh activation (an activation that was popular in the early days of neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc599841",
   "metadata": {},
   "source": [
    "## Reusable function for creating Keras Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08258789",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LAYERS = [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )),\n",
    "          Dense(units=32, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "]\n",
    "DEFAULT_OPTIMIZER = optimizers.RMSprop(learning_rate=0.001)\n",
    "DEFAULT_LOSS = losses.binary_crossentropy\n",
    "DEFAULT_METRICS = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67dbbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(scaled_train_samples, \n",
    "                          train_labels,\n",
    "                          layers = DEFAULT_LAYERS, \n",
    "                          optimizer=DEFAULT_OPTIMIZER,\n",
    "                          loss=DEFAULT_LOSS,\n",
    "                          metrics=DEFAULT_METRICS):\n",
    "    model = Sequential(layers)\n",
    "    model.compile(optimizer=optimizer, loss=loss,metrics=metrics)\n",
    "    model.fit(x=scaled_train_samples, y=train_labels, batch_size=20, epochs=30, verbose=0)\n",
    "    return model\n",
    "\n",
    "def get_nn_results(model, scaled_test_samples, test_labels):\n",
    "    return model.evaluate(scaled_test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7a163",
   "metadata": {},
   "source": [
    "## Further Experiments: Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb03dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {\n",
    "    # 1 hidden layer\n",
    "    '1': [\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 2 hidden layers\n",
    "    '2': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 3 hidden layers\n",
    "    '3': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ]\n",
    "}\n",
    "results_layers_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad86f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8641\n",
      "6/6 [==============================] - 0s 940us/step - loss: 0.3684 - accuracy: 0.8696\n",
      "6/6 [==============================] - 0s 968us/step - loss: 0.3511 - accuracy: 0.8587\n",
      "\n",
      "\n",
      "Achieved 0.3875 loss 0.8641 accuracy with 1 layer(s)\n",
      "Achieved 0.3684 loss 0.8696 accuracy with 2 layer(s)\n",
      "Achieved 0.3511 loss 0.8587 accuracy with 3 layer(s)\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "# converts the numerical values to all be within the range of [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=[0,1])\n",
    "# fit and transform train data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# transform but not fit test data to prevent test data bias leakage\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "# verify the data was transformed\n",
    "scaled_train_samples, scaled_train_samples.shape, scaled_test_samples, scaled_test_samples.shape\n",
    "\n",
    "for key, layers in layers_dict.items():\n",
    "    curr_model = create_neural_network(scaled_train_samples, train_labels, layers)\n",
    "    results_layers_dict[key] = get_nn_results(curr_model, scaled_test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_layers_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with {key} layer(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4853c82",
   "metadata": {},
   "source": [
    "## Further Experiments: Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a94d7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dict = {\n",
    "    # 16 hidden layer\n",
    "    '16': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 32 hidden layers\n",
    "    '32': [\n",
    "          Dense(units=32, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=32, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # 64 hidden layers\n",
    "    '64': [\n",
    "          Dense(units=64, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=64, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ]\n",
    "}\n",
    "results_units_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ba605e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8587\n",
      "6/6 [==============================] - 0s 816us/step - loss: 0.3748 - accuracy: 0.8370\n",
      "6/6 [==============================] - 0s 846us/step - loss: 0.3888 - accuracy: 0.8370\n",
      "\n",
      "\n",
      "Achieved 0.3605 loss 0.8587 accuracy with 16 units/nodes per deep layer\n",
      "Achieved 0.3748 loss 0.8370 accuracy with 32 units/nodes per deep layer\n",
      "Achieved 0.3888 loss 0.8370 accuracy with 64 units/nodes per deep layer\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "# converts the numerical values to all be within the range of [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=[0,1])\n",
    "# fit and transform train data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# transform but not fit test data to prevent test data bias leakage\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "# verify the data was transformed\n",
    "scaled_train_samples, scaled_train_samples.shape, scaled_test_samples, scaled_test_samples.shape\n",
    "\n",
    "for key, layers in units_dict.items():\n",
    "    curr_model = create_neural_network(scaled_train_samples, train_labels, layers)\n",
    "    results_units_dict[key] = get_nn_results(curr_model, scaled_test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_units_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with {key} units/nodes per deep layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6a0ac",
   "metadata": {},
   "source": [
    "## Further Experiments: Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b4bbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = ['binary_crossentropy', 'hinge', 'squared_hinge']\n",
    "results_loss_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf29e30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 854us/step - loss: 0.3638 - accuracy: 0.8696\n",
      "6/6 [==============================] - 0s 916us/step - loss: 0.6380 - accuracy: 0.8478\n",
      "6/6 [==============================] - 0s 739us/step - loss: 0.7288 - accuracy: 0.8370\n",
      "\n",
      "\n",
      "Achieved 0.3638 loss 0.8696 accuracy with the binary_crossentropy loss function\n",
      "Achieved 0.6380 loss 0.8478 accuracy with the hinge loss function\n",
      "Achieved 0.7288 loss 0.8370 accuracy with the squared_hinge loss function\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "\n",
    "# converts the numerical values to all be within the range of [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=[0,1])\n",
    "# fit and transform train data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# transform but not fit test data to prevent test data bias leakage\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "# verify the data was transformed\n",
    "scaled_train_samples, scaled_train_samples.shape, scaled_test_samples, scaled_test_samples.shape\n",
    "\n",
    "for loss_func in loss_functions:\n",
    "    curr_model = create_neural_network(scaled_train_samples, train_labels, loss=loss_func)\n",
    "    results_loss_dict[loss_func] = get_nn_results(curr_model, scaled_test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_loss_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with the {key} loss function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a1d19",
   "metadata": {},
   "source": [
    "## Changing feature range to be [-1,1] for tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0f2d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 843us/step - loss: 0.4098 - accuracy: 0.8533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tanh': [0.40980401635169983, 0.85326087474823]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 2\n",
    "tanh_loss_dict = {}\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "layers = [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )),\n",
    "          Dense(units=32, activation='relu'),\n",
    "          Dense(units=1, activation='tanh')\n",
    "]\n",
    "\n",
    "# converts the numerical values to all be within the range of [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=[-1,1])\n",
    "# fit and transform train data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# transform but not fit test data to prevent test data bias leakage\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "# verify the data was transformed\n",
    "scaled_train_samples, scaled_train_samples.shape, scaled_test_samples, scaled_test_samples.shape\n",
    "\n",
    "curr_model = create_neural_network(scaled_train_samples, train_labels, loss=losses.binary_crossentropy)\n",
    "tanh_loss_dict['tanh'] = get_nn_results(curr_model, scaled_test_samples, test_labels)\n",
    "\n",
    "tanh_loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884a3b",
   "metadata": {},
   "source": [
    "## Further Experiments: Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b29a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions_dict = {\n",
    "    # sigmoid\n",
    "    'sigmoid': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='sigmoid')\n",
    "         ],\n",
    "    # tanh\n",
    "    'tanh': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='tanh')\n",
    "         ],\n",
    "    # relu\n",
    "    'relu': [\n",
    "          Dense(units=16, activation='relu', input_shape=(20, )), \n",
    "          Dense(units=16, activation='relu'),\n",
    "          Dense(units=1, activation='relu')\n",
    "         ]\n",
    "}\n",
    "results_activation_functions_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a983c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 792us/step - loss: 0.3668 - accuracy: 0.8533\n",
      "6/6 [==============================] - 0s 721us/step - loss: 0.4986 - accuracy: 0.8207\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7425 - accuracy: 0.8370\n",
      "\n",
      "\n",
      "Achieved 0.3668 loss 0.8533 accuracy with the sigmoid activation function.\n",
      "Achieved 0.4986 loss 0.8207 accuracy with the tanh activation function.\n",
      "Achieved 0.7425 loss 0.8370 accuracy with the relu activation function.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "\n",
    "train_samples, test_samples, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# convert the dataframes to numpy arrays (tensors)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "\n",
    "# converts the numerical values to all be within the range of [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=[0,1])\n",
    "# fit and transform train data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# transform but not fit test data to prevent test data bias leakage\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "# verify the data was transformed\n",
    "scaled_train_samples, scaled_train_samples.shape, scaled_test_samples, scaled_test_samples.shape\n",
    "\n",
    "for key, layers in activation_functions_dict.items():\n",
    "    curr_model = create_neural_network(scaled_train_samples, train_labels, layers)\n",
    "    results_activation_functions_dict[key] = get_nn_results(curr_model, scaled_test_samples, test_labels)\n",
    "\n",
    "print('\\n')\n",
    "for key, value in results_activation_functions_dict.items():\n",
    "    print(f'Achieved {value[0]:.4f} loss {value[1]:.4f} accuracy with the {key} activation function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c965e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
